{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 //\
//\
//  ViewController.swift\
//  CoreML in ARKit\
//  Created by Seth Nuzum (03-04-18)\
//  Copyright \'a9 2018 Seth Nuzum MK.2 V1\
//\
//\
\
\
\
import UIKit\
import SceneKit\
import ARKit\
\
import Vision\
\
class ViewController: UIViewController, ARSCNViewDelegate \{\
\
    // Scene\
    @IBOutlet var sceneView: ARSCNView!\
    let bubbleDepth : Float = 0.03 // the 'depth' of 3D text returned for visual classifation unit\
    var latestPrediction : String = "\'85" // variable containing the highest probabilty result\
\
\
    var visionRequests = [VNRequest]()\
    let dispatchQueueML = DispatchQueue(label: "com.hw.dispatchqueueml")\
    override func viewDidLoad() \{\
\
        super.viewDidLoad()\
\
\
        sceneView.delegate = self\
\
\
        sceneView.showsStatistics = true\
\
\
        let scene = SCNScene()\
\
\
        sceneView.scene = scene\
\
\
        sceneView.autoenablesDefaultLighting = true\
\
\
        // Tap Gesture Recognizer (this can be modified or completely replaced according to image rec. feedback as required for the application at hand\
        let tapGesture = UITapGestureRecognizer(target: self, action: #selector(self.handleTap(gestureRecognize:)))\
        view.addGestureRecognizer(tapGesture)\
\
\
\
        // Set up Vision Model and import into asset section in order to initiate application\
\
        guard let selectedModel = try? VNCoreMLModel(for: Resnet50().model) else \{ // (Optional) This can be replaced with other models on https://developer.apple.com/machine-learning/\
            fatalError("Could not load model. Ensure model has been drag and dropped (copied) to XCode Project from https://developer.apple.com/machine-learning/ . Also ensure the model is part of a target (see: https://stackoverflow.com/questions/45884085/model-is-not-part-of-any-target-add-the-model-to-a-target-to-enable-generation ")\
        \}\
\
        // Set up Vision-CoreML Request\
        let classificationRequest = VNCoreMLRequest(model: selectedModel, completionHandler: classificationCompleteHandler)\
        classificationRequest.imageCropAndScaleOption = VNImageCropAndScaleOption.centerCrop // Crop from centre of images and scale to appropriate size.\
        visionRequests = [classificationRequest]\
\
        // Begin Loop to Update CoreML\
        loopCoreMLUpdate()\
    \}\
\
    override func viewWillAppear(_ animated: Bool) \{\
        super.viewWillAppear(animated)\
\
        // Create a session configuration\
        let configuration = ARWorldTrackingConfiguration()\
        // Enable plane detection\
        configuration.planeDetection = .horizontal\
\
        // Run the view's session\
        sceneView.session.run(configuration)\
    \}\
\
    override func viewWillDisappear(_ animated: Bool) \{\
        super.viewWillDisappear(animated)\
\
        // Pause the view's session\
        sceneView.session.pause()\
    \}\
\
    override func didReceiveMemoryWarning() \{\
        super.didReceiveMemoryWarning()\
        // Release any cached data, images, etc that aren't in use.\
    \}\
\
    // MARK: - ARSCNViewDelegate\
\
    func renderer(_ renderer: SCNSceneRenderer, updateAtTime time: TimeInterval) \{\
        DispatchQueue.main.async \{\
            // Do any desired updates to SceneKit here.\
        \}\
    \}\
\
    // MARK: - Status Bar: Hide\
    override var prefersStatusBarHidden : Bool \{\
        return true\
    \}\
\
    // MARK: - Interaction\
\
    @objc func handleTap(gestureRecognize: UITapGestureRecognizer) \{\
        // HIT TEST : REAL WORLD\
        // Get Screen Centre\
        let screenCentre : CGPoint = CGPoint(x: self.sceneView.bounds.midX, y: self.sceneView.bounds.midY)\
\
        let arHitTestResults : [ARHitTestResult] = sceneView.hitTest(screenCentre, types: [.featurePoint]) // Alternatively, you could use '.existingPlaneUsingExtent' for more grounded hit-test-points.\
\
        if let closestResult = arHitTestResults.first \{\
            // Get Coordinates of HitTest\
            let transform : matrix_float4x4 = closestResult.worldTransform\
            let worldCoord : SCNVector3 = SCNVector3Make(transform.columns.3.x, transform.columns.3.y, transform.columns.3.z)\
\
            // Create 3D Text\
            let node : SCNNode = createNewBubbleParentNode(latestPrediction)\
            sceneView.scene.rootNode.addChildNode(node)\
            node.position = worldCoord\
        \}\
    \}\
\
    func createNewBubbleParentNode(_ text : String) -> SCNNode \{\
        // Warning: Creating 3D Text is more susceptible to crashing. To reduce chances of crashing, reduce number of polygons, letters, smoothness, etc. This also lowers CPU taxatoin level\
\
        // TEXT BILLBOARD CONSTRAINT\
        let billboardConstraint = SCNBillboardConstraint()\
        billboardConstraint.freeAxes = SCNBillboardAxis.Y\
\
\
\
        // BUBBLE-TEXT\
        let bubble = SCNText(string: text, extrusionDepth: CGFloat(bubbleDepth))\
\
        //Other fonts can be selected in this step if desired\
        var font = UIFont(name: "Futura", size: 0.138)\
        font = font?.withTraits(traits: .traitBold)\
        bubble.font = font\
        bubble.alignmentMode = kCAAlignmentCenter\
        bubble.firstMaterial?.diffuse.contents = UIColor.white\
        bubble.firstMaterial?.specular.contents = UIColor.black\
        bubble.firstMaterial?.isDoubleSided = true\
\
        bubble.chamferRadius = CGFloat(bubbleDepth)\
\
        // BUBBLE NODE\
        let (minBound, maxBound) = bubble.boundingBox\
        let bubbleNode = SCNNode(geometry: bubble)\
        // Centre Node - to Centre-Bottom point\
        bubbleNode.pivot = SCNMatrix4MakeTranslation( (maxBound.x - minBound.x)/2, minBound.y, bubbleDepth/2)\
        // Reduce default text size\
        bubbleNode.scale = SCNVector3Make(0.2, 0.2, 0.2)\
\
        // CENTRE POINT NODE\
        let sphere = SCNSphere(radius: 0.005)\
        sphere.firstMaterial?.diffuse.contents = UIColor.cyan\
        let sphereNode = SCNNode(geometry: sphere)\
\
        // BUBBLE PARENT NODE\
        let bubbleNodeParent = SCNNode()\
        bubbleNodeParent.addChildNode(bubbleNode)\
        bubbleNodeParent.addChildNode(sphereNode)\
        bubbleNodeParent.constraints = [billboardConstraint]\
\
        return bubbleNodeParent\
    \}\
\
\
    func loopCoreMLUpdate() \{\
        // Continuously run CoreML whenever it's ready. (Preventing 'hiccups' in Frame Rate)\
\
        dispatchQueueML.async \{\
            // 1. Run Update.\
            self.updateCoreML()\
\
            // 2. Loop this function.\
            self.loopCoreMLUpdate()\
        \}\
\
    \}\
\
    func classificationCompleteHandler(request: VNRequest, error: Error?) \{\
        // Catch Errors\
        if error != nil \{\
            print("Error: " + (error?.localizedDescription)!)\
            return\
        \}\
        guard let observations = request.results else \{\
            print("No results")\
            return\
        \}\
\
        // Get Classifications\
        let classifications = observations[0...1] // top 2 results\
            .compactMap(\{ $0 as? VNClassificationObservation \})\
            .map(\{ "\\($0.identifier) \\(String(format:"- %.2f", $0.confidence))" \})\
            .joined(separator: "\\n")\
\
\
        DispatchQueue.main.async \{\
            // Print Classifications\
            print(classifications)\
            print("--")\
\
            // Display Debug Text on screen\
            var debugText:String = ""\
            debugText += classifications\
\
\
            // Store the latest prediction\
            var objectName:String = "\'85"\
            objectName = classifications.components(separatedBy: "-")[0]\
            objectName = objectName.components(separatedBy: ",")[0]\
            self.latestPrediction = objectName\
\
        \}\
    \}\
\
    func updateCoreML() \{\
        // Get Camera Image as RGB\
        let pixbuff : CVPixelBuffer? = (sceneView.session.currentFrame?.capturedImage)\
        if pixbuff == nil \{ return \}\
        let ciImage = CIImage(cvPixelBuffer: pixbuff!)\
\
\
        let imageRequestHandler = VNImageRequestHandler(ciImage: ciImage, options: [:])\
\
        \
        // Run Initial Image Request\
        do \{\
            try imageRequestHandler.perform(self.visionRequests)\
        \} catch \{\
            print(error)\
        \}\
\
    \}\
\}\
\
extension UIFont \{\
\
    func withTraits(traits:UIFontDescriptorSymbolicTraits...) -> UIFont \{\
        let descriptor = self.fontDescriptor.withSymbolicTraits(UIFontDescriptorSymbolicTraits(traits))\
        return UIFont(descriptor: descriptor!, size: 0)\
    \}\
\}\
\
\
\
// Any queries regarding product installation, module reference issues/ importation hickups, or image proccessing rate decrese issues, email me at seth141592@gmail.com and I will respond within 48 hours}